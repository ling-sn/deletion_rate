{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebdf09f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.stats import fisher_exact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d48ade8",
   "metadata": {},
   "source": [
    "### Class initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f88fa575",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilterTSV:\n",
    "   def create_mask(self, df, colnames):\n",
    "      \"\"\"\n",
    "      NOTES:\n",
    "      * Select columns that contain \"Deletions\" and put them in a list\n",
    "      * Use set() to remove duplicates, since sets can only contain unique vals\n",
    "      * Pass column names in list to dataframe to create a mask that drops rows\n",
    "        where Deletions == 0 and there are nulls\n",
    "      \"\"\"\n",
    "      del_list = set([col for col in colnames if re.search(r\"Deletions\", col)])\n",
    "      mask = (df[del_list] != 0).all(axis=1) & (~df.isnull().any(axis=1))\n",
    "      return mask\n",
    "\n",
    "   def merged_output(self, df_merged, rep_list, pattern_dict):\n",
    "      \"\"\"\n",
    "      PURPOSE:\n",
    "      1. Takes all columns from merged df and organizes them by BS/NBS type \n",
    "      2. Sums up corresponding bases/deletions & creates 4 new columns per replicate\n",
    "      3. Selects the new columns\n",
    "         * Reshapes each row into 2x2 matrix\n",
    "         * Runs Fisher's Exact Test\n",
    "         * Appends p-val column\n",
    "      \"\"\"\n",
    "      try:\n",
    "         for rep in rep_list:\n",
    "            ## Define names of summed BS/NBS columns\n",
    "            new_cols = [f\"{rep}_TotalBases_BS\", \n",
    "                        f\"{rep}_TotalBases_NBS\"]\n",
    "            \n",
    "            ## Define generic entries for 2x2 contingency table\n",
    "            fisher_cols = [f\"{rep}_TotalBases_BS\", \n",
    "                           f\"{rep}_Deletions_BS\", \n",
    "                           f\"{rep}_TotalBases_NBS\", \n",
    "                           f\"{rep}_Deletions_NBS\"]\n",
    "\n",
    "            ## Calculate p-values\n",
    "            \"\"\"\n",
    "            PART I: For each replicate, find total bases for BS/NBS\n",
    "            * Zips pattern_dict and new_cols together\n",
    "              -> Reminder: pattern_dict = {[List of BS colnames], [List of NBS colnames]}\n",
    "                           new_cols = [\"TotalBases_BS\", \"TotalBases_NBS\"]\n",
    "            * Sum of all entries in 1st list of pattern_dict \n",
    "              -> Stored under 1st colname in new_cols\n",
    "            * Sum of all entries in 2nd list of pattern_dict \n",
    "              -> Stored under 2nd colname in new_cols \n",
    "            \"\"\"\n",
    "            for col, key in zip(new_cols, pattern_dict):\n",
    "               if col not in df_merged.columns:\n",
    "                  df_merged[col] = df_merged[pattern_dict[key]].sum(axis=1)\n",
    "\n",
    "            \"\"\"\n",
    "            PART II: Run Fisher's Exact Test using 2x2 table\n",
    "            * For each row in df_merged:\n",
    "              -> Select the specified columns from fisher_cols\n",
    "              -> Reshape the 4 columns into separate 3D arrays of size 2x2\n",
    "                 (these will be our 2x2 tables)\n",
    "              -> Use these arrays for numpy batch processing\n",
    "            * Run Fisher's Exact Test (scipy) on each table, then select second result\n",
    "              of test AKA the p-val using 'fisher_exact(table)[1]'\n",
    "            \"\"\"\n",
    "            if set(fisher_cols).issubset(df_merged.columns):\n",
    "               df_merged = df_merged.dropna(subset = fisher_cols)\n",
    "               arr = df_merged[fisher_cols].values.reshape(-1, 2, 2) \n",
    "               pvals = [fisher_exact(table)[1] for table in arr]\n",
    "               df_merged[f\"{rep}_Pvalue\"] = pvals\n",
    "         return df_merged\n",
    "      except Exception as e:\n",
    "         print(f\"Failed to calculate p-value for {rep}: {e}\")\n",
    "         traceback.print_exc()\n",
    "         raise\n",
    "\n",
    "   def filter_means(self, df_filtered, colname, cols):\n",
    "      \"\"\"\n",
    "      PURPOSE:\n",
    "      * Use to filter by average (Cutoffs #4-6)\n",
    "      \"\"\"\n",
    "      ## Calculate average and standard deviation\n",
    "      df_filtered[colname] = df_filtered[cols].mean(axis = 1)      \n",
    "      std_colname = colname.replace(\"Avg\", \"Std\")\n",
    "      df_filtered[std_colname] = df_filtered[cols].std(axis = 1)\n",
    "\n",
    "      ## Sort by descending DeletionRate\n",
    "      df_filtered = df_filtered.sort_values(by = colname, \n",
    "                                            ascending = False)\n",
    "\n",
    "      ## If BS, apply filters to average columns\n",
    "      if \"_BS\" in colname:\n",
    "         if \"DeletionCt\" in colname:\n",
    "            df_filtered[colname] = df_filtered[colname].ge(5)\n",
    "         elif \"DeletionRate\" in colname:\n",
    "            df_filtered[colname] = df_filtered[colname].ge(0.02)\n",
    "\n",
    "      return df_filtered\n",
    "\n",
    "   def filtered_output(self, df_merged, rep_list):\n",
    "      \"\"\"\n",
    "      PURPOSE:\n",
    "      a) Adds cutoffs from BID-Pipe protocol:\n",
    "         1. Pvalue across all replicates <= 0.0001\n",
    "         2. RealRate across all replicates >= 0.3\n",
    "         3. Total sequencing coverage for each BS and NBS replicate >= 20\n",
    "         4. Average Deletions across all BS replicates >= 5\n",
    "         5. Average DeletionRate across all BS replicates >= 0.02\n",
    "         6. Average DeletionRate is 2x higher in BS compared to NBS\n",
    "      b) Saves filtered and discarded rows in separate dataframes\n",
    "      \"\"\"\n",
    "      try:\n",
    "         ## Cutoff 1: Pvalue\n",
    "         pval_list = [col for col in df_merged.columns \n",
    "                      if re.search(r\"Pvalue$\", col)]\n",
    "         cutoff1 = df_merged[pval_list].le(0.0001).all(axis=1)\n",
    "         df_filtered = df_merged.loc[cutoff1]\n",
    "\n",
    "         ## Cutoff 2: RealRate\n",
    "         realrate_list = [col for col in df_filtered.columns \n",
    "                          if re.search(r\"RealRate\", col)]\n",
    "         cutoff2 = df_filtered[realrate_list].ge(0.3).all(axis=1)\n",
    "         df_filtered = df_filtered.loc[cutoff2]\n",
    "\n",
    "         ## Cutoff 3: Total sequencing coverage\n",
    "         for rep in rep_list:\n",
    "            for sample in [\"BS\", \"NBS\"]:\n",
    "               coverage_list = [col for col in df_filtered.columns if \n",
    "                                re.match(fr\"{rep}_(TotalBases|Deletions)_{sample}\", col)]\n",
    "               total_sum = df_filtered[coverage_list].sum(axis = 1)\n",
    "               cutoff3 = total_sum.ge(20)\n",
    "               df_filtered = df_filtered.loc[cutoff3]\n",
    "\n",
    "         ## Cutoff 4: Average Deletions (BS)\n",
    "         avg_del_bs = \"AvgDeletionCt_BS\"\n",
    "         del_col_bs = [col for col in df_filtered.columns \n",
    "                       if re.search(r\"_Deletions_BS$*\", col)]\n",
    "         df_filtered = self.filter_means(df_filtered, avg_del_bs, del_col_bs)\n",
    "\n",
    "         ## Cutoff 5: Average DeletionRate (BS)\n",
    "         avg_dr_bs = \"AvgDeletionRate_BS\"\n",
    "         dr_col_bs = [col for col in df_filtered.columns \n",
    "                      if re.search(r\"_DeletionRate_BS$*\", col)]\n",
    "         df_filtered = self.filter_means(df_filtered, avg_dr_bs, dr_col_bs)\n",
    "\n",
    "         ## Cutoff 6: Average DeletionRate is 2x higher in BS compared to NBS\n",
    "         avg_dr_nbs = \"AvgDeletionRate_NBS\"\n",
    "         dr_col_nbs = [col for col in df_filtered.columns \n",
    "                       if re.search(r\"_DeletionRate_NBS$*\", col)]\n",
    "         df_filtered = self.filter_means(df_filtered, avg_dr_nbs, dr_col_nbs)\n",
    "         \n",
    "         cutoff6 = df_filtered[avg_dr_bs] >= 2 * df_filtered[avg_dr_nbs]\n",
    "         df_filtered = df_filtered[cutoff6]\n",
    "\n",
    "         print(\"Successfully applied cutoffs.\")\n",
    "\n",
    "         return df_filtered\n",
    "      \n",
    "      except Exception as e:\n",
    "         print(f\"Failed to apply cutoffs from BID-Pipe protocol: {e}\")\n",
    "         traceback.print_exc()\n",
    "         raise\n",
    "\n",
    "filtertsv = FilterTSV()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7213f1ce",
   "metadata": {},
   "source": [
    "### Loading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a82247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = Path.cwd()\n",
    "input_dir = current_path/\"calculations\"\n",
    "filtertsv = FilterTSV()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02ce8df",
   "metadata": {},
   "source": [
    "### Testing iterative merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11a933a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:45: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:45: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\Sonia Ling\\AppData\\Local\\Temp\\ipykernel_13940\\1842191706.py:45: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Passing a set as an indexer is not supported. Use a list instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m df1_colnames \u001b[38;5;241m=\u001b[39m df_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     23\u001b[0m selected_colnames \u001b[38;5;241m=\u001b[39m df1_colnames[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m17\u001b[39m]\n\u001b[1;32m---> 24\u001b[0m init_mask \u001b[38;5;241m=\u001b[39m filtertsv\u001b[38;5;241m.\u001b[39mcreate_mask(df_list[\u001b[38;5;241m0\u001b[39m], df1_colnames)\n\u001b[0;32m     25\u001b[0m df_full \u001b[38;5;241m=\u001b[39m df_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mloc[init_mask]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03mPART II: Iteratively merge remaining dfs\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m* The range [1:] means 'Start from 2nd item in list, and continue \u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    looping until you reach the end'\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m* Output is df_full, which contains rows from all merged dfs\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m, in \u001b[0;36mFilterTSV.create_mask\u001b[1;34m(self, df, colnames)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mNOTES:\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m* Select columns that contain \"Deletions\" and put them in a list\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m  where Deletions == 0 and there are nulls\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m del_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m colnames \u001b[38;5;28;01mif\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeletions\u001b[39m\u001b[38;5;124m\"\u001b[39m, col)])\n\u001b[1;32m---> 11\u001b[0m mask \u001b[38;5;241m=\u001b[39m (df[del_list] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mall(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m (\u001b[38;5;241m~\u001b[39mdf\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39many(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mask\n",
      "File \u001b[1;32mc:\\Users\\Sonia Ling\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4063\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4062\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m-> 4063\u001b[0m     check_dict_or_set_indexers(key)\n\u001b[0;32m   4064\u001b[0m     key \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mitem_from_zerodim(key)\n\u001b[0;32m   4065\u001b[0m     key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Sonia Ling\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:2774\u001b[0m, in \u001b[0;36mcheck_dict_or_set_indexers\u001b[1;34m(key)\u001b[0m\n\u001b[0;32m   2766\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2767\u001b[0m \u001b[38;5;124;03mCheck if the indexer is or contains a dict or set, which is no longer allowed.\u001b[39;00m\n\u001b[0;32m   2768\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2770\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mset\u001b[39m)\n\u001b[0;32m   2771\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[0;32m   2772\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mset\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[0;32m   2773\u001b[0m ):\n\u001b[1;32m-> 2774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   2775\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a set as an indexer is not supported. Use a list instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2776\u001b[0m     )\n\u001b[0;32m   2778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2779\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m   2780\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[0;32m   2781\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[0;32m   2782\u001b[0m ):\n\u001b[0;32m   2783\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   2784\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a dict as an indexer is not supported. Use a list instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2785\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Passing a set as an indexer is not supported. Use a list instead."
     ]
    }
   ],
   "source": [
    "for subfolder in input_dir.iterdir():\n",
    "    tsv_folder = input_dir/subfolder/\"individual_tsv\"\n",
    "    processed_folder = current_path/\"calculations\"/subfolder\n",
    "\n",
    "    if subfolder.is_dir():\n",
    "    ## Collect paths of .tsv files and put in list\n",
    "        tsv_list = sorted(\n",
    "        tsv_folder.glob(\"*.tsv\"),\n",
    "        key = lambda x: int(re.search(r\"Rep(\\d+)\", x.name).group(1)) ## order by rep integer\n",
    "    ) \n",
    "\n",
    "    ## Read in TSVs\n",
    "    df_list = [pd.read_csv(str(file), sep = \"\\t\") for file in tsv_list]\n",
    "\n",
    "    ## Merge pandas dataframes\n",
    "    \"\"\"\n",
    "    PART I: Create initial df_full w/ df1\n",
    "    * df_list[0] = df1\n",
    "    * Select out column names that are always the same throughout all dfs\n",
    "    * Use create_mask() to drop null rows & rows where \"Deletions\" == 0\n",
    "    \"\"\"\n",
    "    df1_colnames = df_list[0].columns.tolist()\n",
    "    selected_colnames = df1_colnames[0:17]\n",
    "    init_mask = filtertsv.create_mask(df_list[0], df1_colnames)\n",
    "    df_full = df_list[0].loc[init_mask]\n",
    "\n",
    "    \"\"\"\n",
    "    PART II: Iteratively merge remaining dfs\n",
    "    * The range [1:] means 'Start from 2nd item in list, and continue \n",
    "        looping until you reach the end'\n",
    "    * Output is df_full, which contains rows from all merged dfs\n",
    "    \"\"\"\n",
    "    for i in df_list[1:]:\n",
    "        colnames = df_list[i].columns.tolist()\n",
    "        mask = filtertsv.create_mask(df_list[i], colnames)\n",
    "        df_list[i] = df_list[i].loc[mask]\n",
    "        df_full = pd.merge(df_full, df_list[i], on = selected_colnames, how = \"outer\")\n",
    "\n",
    "    ## Sort column names\n",
    "    \"\"\"\n",
    "    PART I: Collect all column names\n",
    "    \"\"\"\n",
    "    merged_colnames = df_full.columns.tolist()\n",
    "\n",
    "    \"\"\"\n",
    "    PART II: Sort by replicate order\n",
    "    * Select columns that contain \"Rep\\d+\", such as Rep1, Rep2, Rep3, etc.,\n",
    "        and put them in a list\n",
    "    * Use set() to remove duplicates, since sets can only contain unique vals\n",
    "    * On those columns, select the first RegEx match group. In this case,\n",
    "        it'd be \"\\d+\" or the digit, such as 1, 2, 3, etc.\n",
    "    * Sort columns by those digits in ascending order using sorted() \n",
    "    \"\"\"\n",
    "    rep_list = sorted(\n",
    "        set([re.search(r\"(Rep\\d+)\", col).group(1) for col in merged_colnames \n",
    "            if re.search(r\"(Rep\\d+)\", col)]), \n",
    "            key = lambda x: int(re.search(r\"Rep(\\d+)\", x).group(1))\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    NOTES:\n",
    "    * Drop nulls to ensure deletion rates for all 3 replicates\n",
    "    * Calculate p-vals with Fisher's Exact Test\n",
    "    * Sort DeletionRate by descending order\n",
    "    * Keep only first 50 rows\n",
    "    \"\"\"\n",
    "    ## Group corresponding BS/NBS into separate lists\n",
    "    df_merged = df_full.dropna()\n",
    "    for rep in rep_list:\n",
    "        bs_base_pattern = re.compile(fr\"{rep}_(A|C|G|T)_BS$\")\n",
    "        nbs_base_pattern = re.compile(fr\"{rep}_(A|C|G|T)_NBS$\")\n",
    "        pattern_dict = {f\"{rep}_Bases_BS\": [col for col in merged_colnames \n",
    "                                            if bs_base_pattern.match(col)],\n",
    "                        f\"{rep}_Bases_NBS\": [col for col in merged_colnames \n",
    "                                            if nbs_base_pattern.match(col)]}\n",
    "\n",
    "    ## Run Fisher's Exact Test (p-values)\n",
    "    filtertsv.merged_output(df_merged, rep_list, pattern_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
