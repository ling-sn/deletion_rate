{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ef9f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use RNA-STAR conda environment\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pysam\n",
    "import concurrent.futures\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33b5bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pysam_pileup(bamfile, chrom, mod_base, base_ct):\n",
    "    \"\"\"\n",
    "    Counts number of bases/deletions in PileupColumn \n",
    "    for each GenomicModBase\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for pileupcolumn in bamfile.pileup(str(chrom), int(mod_base-1), int(mod_base), truncate = True): ## pysam is 0-based, while GenomicModBase was 1-based\n",
    "            pileupcolumn.set_min_base_quality(0) ## prevent pysam from filtering based on base quality\n",
    "            if pileupcolumn.pos == int(mod_base-1):\n",
    "                for pileupread in pileupcolumn.pileups:\n",
    "                    if pileupread.is_del and not pileupread.is_refskip:\n",
    "                        base_ct[\"Deletions\"] += 1\n",
    "                    elif not pileupread.is_refskip:\n",
    "                        base = pileupread.alignment.query_sequence[pileupread.query_position] ## taken from example in manual; returns base letter\n",
    "                        if base in base_ct:\n",
    "                            base_ct[base] += 1\n",
    "                        else:\n",
    "                            continue\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to count bases/deletions in PileupColumn for Chromosome {chrom} at GenomicModBase {mod_base}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "def count_base(chunk, input_bam_name, results, key):\n",
    "    bamfile = pysam.AlignmentFile(input_bam_name, \"rb\")\n",
    "    \"\"\"\n",
    "    Counts number of bases/deletions for each UNUAR site\n",
    "        1) chrom: value in \"Chrom\" column (e.g., NW_018654708.1)\n",
    "        2) mod_base: value in \"GenomicModBase\" column (e.g., 373)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for row in chunk:\n",
    "            chrom = row[0]\n",
    "            mod_base = row[1]\n",
    "            base_ct = {key[\"A\"]: 0, key[\"C\"]: 0, key[\"G\"]: 0, key[\"T\"]: 0, key[\"Deletions\"]: 0}\n",
    "\n",
    "            pysam_pileup(bamfile, chrom, mod_base, base_ct)\n",
    "\n",
    "            results.append({\"Chrom\": chrom,\n",
    "                            \"GenomicModBase\": mod_base,\n",
    "                            **base_ct}) ## unpack base_ct dict in this dict\n",
    "        bamfile.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to count bases/deletions in UNUAR sites: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "def process_chunk(genome_coord, input_bam_name, results, key):\n",
    "    try:\n",
    "        all_chunks = np.array_split(genome_coord, 100)\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers = 12) as executor:\n",
    "            futures = [executor.submit(count_base, chunk, input_bam_name, results, key) for chunk in all_chunks]\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                future.result()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parallelize chunks: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f126c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_key(subfolder, base_key):\n",
    "    \"\"\"\n",
    "    Modifies names of dictionary keys based on \n",
    "    the Replicate # (1, 2, 3) and Sample Type (BS, NBS)\n",
    "    in a given subfolder name.\n",
    "    \"\"\"\n",
    "    ## Adds replicate prefix to dictionary key names\n",
    "    for rep in [\"Rep1\", \"Rep2\", \"Rep3\"]:\n",
    "        if f\"-{rep}-\" in str(subfolder):\n",
    "            prefix = rep + \"_\"\n",
    "            break\n",
    "    \n",
    "    ## Adds sample type suffix to dictionary key names\n",
    "    for sample in [\"BS\", \"NBS\"]:\n",
    "        if f\"-{sample}_\" in str(subfolder):\n",
    "            suffix = \"_\" + sample\n",
    "            break\n",
    "    \n",
    "    return prefix + base_key + suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05de6045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_regex(folder_name):\n",
    "    \"\"\"\n",
    "    Given input folder names, extract the group name.\n",
    "        EXAMPLE: '7KO-Cyto-BS_processed_fastqs' -> '7KO-Cyto'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        match = re.match(r\"(.+)-(?:BS|NBS)_processed_fastqs\", folder_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to match input folder to group with RegEx: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "    return match.group(1) ## return first regex capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa95dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## main code\n",
    "def open_bam(folder_name):\n",
    "    \"\"\"\n",
    "    Opens .bam in folder\n",
    "    and runs calculations\n",
    "    \"\"\"\n",
    "    current_path = Path.cwd()\n",
    "    input_dir = current_path/\"realignments\"/folder_name\n",
    "    group_name = match_regex(folder_name)\n",
    "    \n",
    "    left = pd.read_csv(Path(\"~/umms-RNAlabDATA/Software/genome_indices/UNUAR_motif_sites_mRNA_hg38p14.tsv\").expanduser(), sep = \"\\t\")\n",
    "    right = pd.read_excel(f\"{current_path}/SupplementaryTable1.xlsx\")\n",
    "\n",
    "    df = pd.merge(left, right, how = \"left\", on = \"Motif\")\n",
    "    genome_coord = df[[\"Chrom\", \"GenomicModBase\"]].to_numpy() ## faster processing\n",
    "    \n",
    "    try: \n",
    "        for subfolder in input_dir.iterdir():\n",
    "            if subfolder.is_dir():\n",
    "                processed_folder = current_path/\"calculations\"/group_name\n",
    "                processed_folder.mkdir(exist_ok=True, parents=True)\n",
    "                \n",
    "                key = {base_key: make_key(subfolder, base_key) for base_key in [\"A\", \"C\", \"G\", \"T\", \"Deletions\"]}\n",
    "                \n",
    "                for bam in subfolder.glob(\"*.bam\"):\n",
    "                    results = []\n",
    "                    input_bam_name = Path(bam) ## turn string from list back into filepath\n",
    "                    output_tsv_name = processed_folder/f\"{input_bam_name.stem}.tsv\"\n",
    "                    \n",
    "                    ## count A, C, G, T and deletions @ each UNUAR site\n",
    "                    process_chunk(genome_coord, input_bam_name, results, key)\n",
    "\n",
    "                    ## calculate observed deletion rates\n",
    "                    counts = pd.DataFrame(results)\n",
    "                    total_sum = counts[[\"A\", \"C\", \"G\", \"T\", \"Deletions\"]].sum(axis = 1) ## sum across rows\n",
    "                    counts[\"DeletionRate\"] = counts[\"Deletions\"]/total_sum\n",
    "                    \n",
    "                    ## calculate real deletion rates\n",
    "                    df_final = pd.merge(df, counts, how = \"left\", on = [\"Chrom\", \"GenomicModBase\"])\n",
    "                    num = df_final[\"fit_b\"] - df_final[\"DeletionRate\"]\n",
    "                    denom = (df_final[\"fit_c\"]*(df_final[\"fit_b\"] + df_final[\"fit_s\"] -\n",
    "                             df_final[\"fit_s\"]*df_final[\"DeletionRate\"]-1))\n",
    "                    df_final[\"RealRate\"] = num/denom\n",
    "\n",
    "                    ## add all calculations to og dataframe & save as .tsv output\n",
    "                    df_final.to_csv(output_tsv_name, sep = \"\\t\", index = False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to calculate observed & real deletion rates in {folder_name} and save as .tsv: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02112d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description = \"Calculates observed and real deletion rates for every UNUAR site in a BAM file.\")\n",
    "    parser.add_argument(\"--folder_name\", help = \"Name of processed_fastqs folder\", required = True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(\"Calculating deletion rates...\")\n",
    "    open_bam(args.folder_name)\n",
    "    print(\"Process finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
