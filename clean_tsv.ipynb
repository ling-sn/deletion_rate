{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b3433",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use RNA-STAR conda environment\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.stats import fisher_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e30fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilterTSV:\n",
    "   def create_mask(self, df, colnames):\n",
    "      ## Search colnames for Deletions -> put in list -> remove duplicates -> sort in ascending order\n",
    "      del_list = sorted(set([col for col in colnames if re.search(r\"Deletions\", col)]))\n",
    "      ## Pass list to dataframe -> only keep rows where Deletions == 0 and there are no nulls\n",
    "      mask = (df[del_list] != 0).all(axis=1) & (~df.isnull().any(axis=1))\n",
    "      return mask\n",
    "\n",
    "   def merged_output(self, df_merged, merged_colnames, rep_list):\n",
    "      \"\"\"\n",
    "      1. Takes all columns from merged df and organizes them by BS/NBS type \n",
    "      2. Sums up corresponding bases and deletions and creates 4 new columns per replicate\n",
    "      3. Selects the new columns\n",
    "         * Reshapes each row into 2x2 matrix\n",
    "         * Runs Fisher's Exact Test\n",
    "         * Appends p-value column\n",
    "      \"\"\"\n",
    "      try:\n",
    "         for rep in rep_list: \n",
    "            bs_base_pattern = re.compile(fr\"{rep}_(A|C|G|T)_BS$\")\n",
    "            nbs_base_pattern = re.compile(fr\"{rep}_(A|C|G|T)_NBS$\")\n",
    "         \n",
    "            pattern_dict = {f\"{rep}_Bases_BS\": [col for col in merged_colnames if bs_base_pattern.match(col)],\n",
    "                            f\"{rep}_Bases_NBS\": [col for col in merged_colnames if nbs_base_pattern.match(col)]}\n",
    "\n",
    "            newcols = [f\"{rep}_TotalBases_BS\",\n",
    "                       f\"{rep}_TotalBases_NBS\"]\n",
    "            \n",
    "            fisher_cols = [f\"{rep}_TotalBases_BS\", \n",
    "                           f\"{rep}_Deletions_BS\", \n",
    "                           f\"{rep}_TotalBases_NBS\", \n",
    "                           f\"{rep}_Deletions_NBS\"]\n",
    "\n",
    "            for col, key in zip(newcols, pattern_dict):\n",
    "               if col not in df_merged.columns:\n",
    "                  df_merged[col] = df_merged[pattern_dict[key]].sum(axis=1) ## col = sum of list of cols from dictionary\n",
    "\n",
    "            if set(fisher_cols).issubset(df_merged.columns):\n",
    "               df_merged = df_merged.dropna(subset=fisher_cols)\n",
    "               df_merged[f\"{rep}_Pvalue\"] = df_merged[fisher_cols].apply(lambda row: fisher_exact(row.values.reshape(2, 2))[1], axis=1) ## each row is reshaped into 2x2 matrix\n",
    "\n",
    "         df_merged = df_merged.drop(columns=[\"index\"]) ## after for loop finishes, drop index column\n",
    "         \n",
    "         return df_merged\n",
    "      except Exception as e:\n",
    "         print(f\"Failed to calculate p-value for {rep}: {e}\")\n",
    "         traceback.print_exc()\n",
    "         raise\n",
    "\n",
    "   def conditional_filter(self, df_filtered, df_dropped, col):\n",
    "      \"\"\" \n",
    "      Use to filter by conditional mean (Cutoffs #4-5)\n",
    "      \"\"\"\n",
    "      min_val = df_filtered[col].min()\n",
    "      df_dropped = pd.concat([df_dropped, df_filtered[df_filtered[col] == min_val]]) ## drop min. value if conditional mean is not satisfied\n",
    "      df_filtered = df_filtered[df_filtered[col] > min_val] ## filter df to exclude min value\n",
    "\n",
    "      return df_filtered, df_dropped\n",
    "\n",
    "   def filtered_output(self, df_merged, rep_list):\n",
    "      \"\"\"\n",
    "      a) Adds cutoffs from BID-Pipe protocol:\n",
    "         1. Pvalue across all replicates < 0.0004\n",
    "         2. RealRate across all replicates > 0.3\n",
    "         3. Total sequencing coverage for each BS|NBS replicate > 20\n",
    "         4. Average Deletions for each BS replicate > 5\n",
    "         5. Average DeletionRate for each BS replicate > 0.02\n",
    "         6. Average DeletionRate is 2x higher in BS replicate compared to NBS replicate\n",
    "      b) Saves filtered and discarded rows in separate dataframes\n",
    "      \"\"\"\n",
    "      try:\n",
    "         ## Cutoff 1: Pvalue\n",
    "         pval_list = [col for col in df_merged.columns if re.search(r\"Pvalue$\", col)]\n",
    "         cutoff1 = df_merged[pval_list].lt(0.0004).all(axis=1)\n",
    "         df_filtered = df_merged.loc[cutoff1]\n",
    "         df_dropped = df_merged.loc[~cutoff1]\n",
    "\n",
    "         ## Cutoff 2: RealRate\n",
    "         realrate_list = [col for col in df_filtered.columns if re.search(r\"RealRate\", col)]\n",
    "         cutoff2 = df_filtered[realrate_list].gt(0.3).all(axis=1)\n",
    "         df_filtered = df_filtered.loc[cutoff2]\n",
    "         df_dropped = pd.concat([df_dropped, df_filtered.loc[~cutoff2]]) ## append dropped rows to existing df\n",
    "\n",
    "         ## Cutoff 3: Total sequencing coverage\n",
    "         for rep in rep_list:\n",
    "            for sample in [\"BS\", \"NBS\"]:\n",
    "               coverage_list = [col for col in df_filtered.columns if re.match(fr\"{rep}_(TotalBases|Deletions)_{sample}\", col)]\n",
    "               total_sum = df_filtered[coverage_list].sum(axis=1)\n",
    "               cutoff3 = total_sum.gt(20)\n",
    "               df_filtered = df_filtered.loc[cutoff3]\n",
    "               df_dropped = pd.concat([df_dropped, df_filtered.loc[~cutoff3]])\n",
    "\n",
    "         ## Cutoff 4: Conditional mean (Deletions)\n",
    "         for rep in rep_list:\n",
    "            del_col = f\"{rep}_Deletions_BS\"\n",
    "            del_mean = df_filtered[del_col].mean()\n",
    "            while del_mean <= 5 and not df_filtered.empty:\n",
    "               df_filtered, df_dropped = self.conditional_filter(df_filtered, df_dropped, del_col)\n",
    "\n",
    "         ## Cutoff 5: Conditional mean (DeletionRate)\n",
    "         for rep in rep_list:\n",
    "            dr_col_bs = f\"{rep}_DeletionRate_BS\"\n",
    "            dr_mean_bs = df_filtered[dr_col_bs].mean()\n",
    "            while dr_mean_bs <= 0.02 and not df_filtered.empty:\n",
    "               self.conditional_filter(dr_col_bs)\n",
    "\n",
    "         ## Cutoff 6: Average DeletionRate is 2x higher in BS replicate compared to NBS replicate\n",
    "         for rep in rep_list:\n",
    "            dr_col_nbs = f\"{rep}_DeletionRate_NBS\"\n",
    "            dr_mean_nbs = df_filtered[dr_col_nbs].mean()\n",
    "            while dr_mean_bs < 2 * dr_mean_nbs:\n",
    "               self.conditional_filter(dr_col_bs)\n",
    "\n",
    "         print(\"Successfully applied cutoffs.\")\n",
    "\n",
    "         return df_filtered, df_dropped\n",
    "      \n",
    "      except Exception as e:\n",
    "         print(f\"Failed to apply cutoffs from BID-Pipe protocol: {e}\")\n",
    "         traceback.print_exc()\n",
    "         raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe8a426",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 13 (1775068193.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[25], line 14\u001b[1;36m\u001b[0m\n\u001b[1;33m    input_folder = current_path/\"calculations\"/group_name/\"individual_tsv\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'if' statement on line 13\n"
     ]
    }
   ],
   "source": [
    "## main code\n",
    "def clean_output():\n",
    "    \"\"\"\n",
    "    Filters .tsv files in grouped folders\n",
    "    \"\"\"\n",
    "    current_path = Path.cwd()\n",
    "    input_dir = current_path/\"calculations\"\n",
    "\n",
    "    ## Initialize class\n",
    "    filtertsv = FilterTSV()\n",
    "\n",
    "    try: \n",
    "        for subfolder in input_dir.iterdir():\n",
    "            tsv_folder = input_dir/subfolder/\"individual_tsv\"\n",
    "            processed_folder = current_path/\"calculations\"/subfolder\n",
    "            \n",
    "            if subfolder.is_dir():\n",
    "                tsv_list = sorted(\n",
    "                    tsv_folder.glob(\"*.tsv\"), ## collect paths of tsv files and put in a list\n",
    "                    key = lambda x: int(re.search(r\"Rep(\\d+)\", x).group(1)) ## order by rep integer \n",
    "                    ) \n",
    "                num = [\"df%s\" %s for s in range(1, len(tsv_list)+1)] ## creates a list of strings: df1, df2, ..., df6\n",
    "                listcomp = [pd.read_csv(i, sep = \"\\t\") for i in tsv_list] ## reads in all tsv files as pandas df; access 1st df w/ listcomp[0], etc.\n",
    "                df_dict = dict(zip(num, listcomp))\n",
    "\n",
    "                ## Merge pandas dataframes\n",
    "                df1_colnames = df_dict[\"df1\"].columns.tolist()\n",
    "                selected_colnames = [\"index\"] + df1_colnames[0:17] ## columns that are always the same throughout all dfs\n",
    "                init_mask = filtertsv.create_mask(df_dict[num[0]], df1_colnames) ## drop \"Deletions\"==0 and null rows\n",
    "                df_full = df_dict[num[0]].loc[init_mask].reset_index() ## create initial df_full w/ df1\n",
    "                df_dropped = df_dict[num[0]].loc[~init_mask].reset_index() ## create initial df_dropped w/ df1\n",
    "\n",
    "                for i in num[1:]:\n",
    "                    colnames = df_dict[i].columns.tolist()\n",
    "                    mask = filtertsv.create_mask(df_dict[i], colnames)\n",
    "                    df_dict[i] = df_dict[i].loc[mask]\n",
    "                    df_dropped = pd.concat([df_dropped, df_dict[i].loc[~mask]])\n",
    "                    df_full = pd.merge(df_full, df_dict[i].reset_index(), on = selected_colnames, how = \"outer\")\n",
    "\n",
    "                ## Collect column and replicate names\n",
    "                merged_colnames = df_full.columns.tolist()\n",
    "\n",
    "                ## Search colnames for Rep(#) -> put in list -> remove duplicates -> sort in ascending order\n",
    "                rep_list = sorted(\n",
    "                    set([re.search(r\"(Rep\\d+)\", col).group(1) for col in merged_colnames if re.search(r\"(Rep\\d+)\", col)]), \n",
    "                    key = lambda x: int(re.search(r\"Rep(\\d+)\", x).group(1)) ## sort by rep digit\n",
    "                    )\n",
    "\n",
    "                ## Save null .tsv (missing_data)\n",
    "                null_rows = df_full.isnull().any(axis=1)\n",
    "                df_null = df_full[null_rows].copy()\n",
    "                df_null.to_csv(f\"{processed_folder}/{subfolder}_missing_data.tsv\", sep = \"\\t\", index = False)\n",
    "\n",
    "                ## Save merged .tsv (all_sites)\n",
    "                df_merged = df_full.dropna() ## p-val calc doesn't work w/ null values\n",
    "                filtertsv.merged_output(df_merged, merged_colnames, rep_list) ## add p-val column\n",
    "                df_merged.to_csv(f\"{processed_folder}/{subfolder}_all_sites.tsv\", sep = \"\\t\", index = False)\n",
    "\n",
    "                ## Save filtered .tsv (filtered)\n",
    "                df_filtered, df_dropped = filtertsv.filtered_output(df_merged, rep_list)\n",
    "                df_filtered.to_csv(f\"{processed_folder}/{subfolder}_filtered.tsv\", sep = \"\\t\", index = False)\n",
    "\n",
    "                ## Save filtered out rows in .tsv (non_pass & non_sites)\n",
    "                # (a) Rows that failed cutoffs (non_pass)\n",
    "                cutoff7 = df_dropped[\"Deletions\"!=0]\n",
    "                df_failcut = df_dropped[cutoff7]\n",
    "                df_failcut.to_csv(f\"{processed_folder}/{subfolder}_non_pass.tsv\", sep = \"\\t\", index = False)\n",
    "                # (b) Rows w/ Deletions==0 (non_site)\n",
    "                df_zerodel = df_dropped.loc[~cutoff7] \n",
    "                df_zerodel.to_csv(f\"{processed_folder}/{subfolder}_non_sites.tsv\", sep = \"\\t\", index = False) \n",
    "\n",
    "                # ## Save priority .tsv (priority_filtered)\n",
    "                # \"\"\"\n",
    "                # Drops redundant columns\n",
    "                # \"\"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create merged .tsv file: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7062ddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Filtering .tsv files...\")\n",
    "    clean_output()\n",
    "    print(\"Process finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
