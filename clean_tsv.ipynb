{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "028b3433",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use RNA-STAR conda environment\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.stats import fisher_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4b15cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_regex(folder_name):\n",
    "    \"\"\"\n",
    "    Given input folder names, extract the group name.\n",
    "        EXAMPLE: '7KO-Cyto-BS_processed_fastqs' -> '7KO-Cyto'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        match = re.match(r\"(.+)-(?:BS|NBS)_processed_fastqs\", folder_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to match input folder to group with RegEx: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "    return match.group(1) ## return first capture group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e30fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilterTSV:\n",
    "   def merged_output(self, df_merged, merged_colnames, rep_list):\n",
    "      \"\"\"\n",
    "      1. Takes all columns from merged df and organizes them by BS/NBS type \n",
    "      2. Sums up corresponding bases and deletions and creates 4 new columns per replicate\n",
    "      3. Selects the new columns\n",
    "         * Reshapes each row into 2x2 matrix\n",
    "         * Runs Fisher's Exact Test\n",
    "         * Appends p-value column\n",
    "      \"\"\"\n",
    "      try:\n",
    "         for rep in rep_list: \n",
    "            bs_base_pattern = re.compile(fr\"{rep}_(A|C|G|T)_BS$\")\n",
    "            nbs_base_pattern = re.compile(fr\"{rep}_(A|C|G|T)_NBS$\")\n",
    "         \n",
    "            pattern_dict = {f\"{rep}_Bases_BS\": [col for col in merged_colnames if bs_base_pattern.match(col)],\n",
    "                            f\"{rep}_Bases_NBS\": [col for col in merged_colnames if nbs_base_pattern.match(col)]}\n",
    "\n",
    "            fisher_cols = [f\"{rep}_TotalBases_BS\", \n",
    "                           f\"{rep}_Deletions_BS\", \n",
    "                           f\"{rep}_TotalBases_NBS\", \n",
    "                           f\"{rep}_Deletions_NBS\"]\n",
    "\n",
    "            for newcol, key in zip(fisher_cols, pattern_dict):\n",
    "               if newcol not in df_merged.columns:\n",
    "                  df_merged[newcol] = df_merged[pattern_dict[key]].sum(axis=1) ## new col = sum of list of cols from dictionary\n",
    "\n",
    "            df_merged[f\"{rep}_Pvalue\"] = df_merged[fisher_cols].apply(lambda row: fisher_exact(row.values.reshape(2, 2))[1], axis=1) ## each row is reshaped into 2x2 matrix\n",
    "         \n",
    "         df_merged = df_merged.drop(columns=[\"index\"]) ## after for loop finishes, drop index column\n",
    "      except Exception as e:\n",
    "         print(f\"Failed to calculate p-value for {rep}: {e}\")\n",
    "         traceback.print_exc()\n",
    "         raise\n",
    "   \n",
    "   def conditional_filter(self, col):\n",
    "      \"\"\" \n",
    "      Use to filter by conditional mean (Cutoffs #4-5)\n",
    "      \"\"\"\n",
    "      min_val = df_filtered[col].min()\n",
    "      df_dropped = pd.concat([df_dropped, df_filtered[df_filtered[col] == min_val]]) ## drop min. value if conditional mean is not satisfied\n",
    "      df_filtered = df_filtered[df_filtered[col] > min_val] ## filter df to exclude min value\n",
    "\n",
    "   def priority_output(self, df_priority, rep_list):\n",
    "      \"\"\"\n",
    "      a) Adds cutoffs from BID-Pipe protocol:\n",
    "         1. Pvalue across all replicates < 0.0004\n",
    "         2. RealRate across all replicates > 0.3\n",
    "         3. Total sequencing coverage for each BS|NBS replicate > 20\n",
    "         4. Average Deletions for each BS replicate > 5\n",
    "         5. Average DeletionRate for each BS replicate > 0.02\n",
    "         6. Average DeletionRate is 2x higher in BS replicate compared to NBS replicate\n",
    "      b) Saves filtered and discarded rows in separate dataframes\n",
    "      \"\"\"\n",
    "      try:\n",
    "         ## Cutoff 1: Pvalue\n",
    "         pval_list = df_priority.columns[df_priority.columns.str.contains(r\"Pvalue$\", regex=True)].tolist()\n",
    "         cutoff1 = df_priority[pval_list].lt(0.0004).all(axis=1)\n",
    "         df_filtered = df_priority[cutoff1]\n",
    "         df_dropped = df_priority[~cutoff1]\n",
    "\n",
    "         ## Cutoff 2: RealRate\n",
    "         realrate_list = df_filtered.columns[df_filtered.columns.str.contains(r\"RealRate\", regex=True)].tolist()\n",
    "         cutoff2 = df_filtered[realrate_list].gt(0.3).all(axis=1)\n",
    "         df_filtered = df_filtered[cutoff2]\n",
    "         df_dropped = pd.concat([df_dropped, df_filtered[~cutoff2]]) ## append dropped rows to existing df\n",
    "\n",
    "         ## Cutoff 3: Total sequencing coverage\n",
    "         for rep in rep_list:\n",
    "            for sample in [\"BS\", \"NBS\"]:\n",
    "               coverage_list = df_filtered.columns[df_filtered.columns.str.contains(fr\"{rep}_(TotalBases|Deletions)_{sample}\", regex=True)].tolist()\n",
    "               total_sum = df_filtered[coverage_list].sum(axis=1)\n",
    "               cutoff3 = total_sum.gt(20)\n",
    "               df_filtered = df_filtered[cutoff3]\n",
    "               df_dropped = pd.concat([df_dropped, df_filtered[~cutoff3]])\n",
    "\n",
    "         ## Cutoff 4: Conditional mean (Deletions)\n",
    "         for rep in rep_list:\n",
    "            del_col = f\"{rep}_Deletions_BS\"\n",
    "            del_mean = df_filtered[del_col].mean()\n",
    "            while del_mean <= 5 and not df_filtered.empty:\n",
    "               self.conditional_filter(del_col)\n",
    "\n",
    "         ## Cutoff 5: Conditional mean (DeletionRate)\n",
    "         for rep in rep_list:\n",
    "            dr_col_bs = f\"{rep}_DeletionRate_BS\"\n",
    "            dr_mean_bs = df_filtered[dr_col_bs].mean()\n",
    "            while dr_mean_bs <= 0.02 and not df_filtered.empty:\n",
    "               self.conditional_filter(dr_col_bs)\n",
    "\n",
    "         ## Cutoff 6: Average DeletionRate is 2x higher in BS replicate compared to NBS replicate\n",
    "         for rep in rep_list:\n",
    "            dr_col_nbs = f\"{rep}_DeletionRate_NBS\"\n",
    "            dr_mean_nbs = df_filtered[dr_col_nbs].mean()\n",
    "            while dr_mean_bs < 2 * dr_mean_nbs:\n",
    "               self.conditional_filter(dr_col_bs)\n",
    "\n",
    "         print(\"Successfully applied cutoffs.\")\n",
    "\n",
    "         return [df_filtered, df_dropped]\n",
    "      \n",
    "      except Exception as e:\n",
    "         print(f\"Failed to apply cutoffs from BID-Pipe protocol: {e}\")\n",
    "         traceback.print_exc()\n",
    "         raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe8a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "## main code\n",
    "def clean_output(folder_name):\n",
    "    \"\"\"\n",
    "    Filters .tsv files in grouped folders\n",
    "    \"\"\"\n",
    "    current_path = Path.cwd()\n",
    "    group_name = match_regex(folder_name)\n",
    "    input_folder = current_path/\"calculations\"/group_name/\"individual_tsv\"\n",
    "    processed_folder = current_path/\"calculations\"/group_name\n",
    "\n",
    "    try: \n",
    "        if input_folder.is_dir():\n",
    "            tsv_list = [*input_folder.glob(\"*.tsv\")] ## collect paths of tsv files and put in a list            \n",
    "            num = [\"df%s\" %s for s in range(1,7)] ## creates a list of strings: df1, df2, ..., df6\n",
    "            listcomp = [pd.read_csv(i, sep = \"\\t\") for i in tsv_list] ## reads in all tsv files as pandas df; access 1st df w/ listcomp[0], etc.\n",
    "            df_dict = dict(zip(num, listcomp))\n",
    "\n",
    "            ## Merge pandas dataframes\n",
    "            colnames = df_dict[\"df1\"].columns.tolist()\n",
    "            selected_colnames = [\"index\"] + colnames[0:17] ## columns that are always the same throughout all dfs\n",
    "\n",
    "            for i in num:\n",
    "                if i==0:\n",
    "                    ## merge df1 + df2\n",
    "                    df_merged = pd.merge(df_dict[i].reset_index(), df_dict[i+1].reset_index(), on = selected_colnames, how = \"outer\")\n",
    "                elif i==5:\n",
    "                    break\n",
    "                else:\n",
    "                    ## for remaining dfs after df1 + df2, overwrite existing var: df_merged = df_merged + df_dict[i+1]\n",
    "                    df_merged = pd.merge(df_merged, df_dict[i+1].reset_index(), on = selected_colnames, how = \"outer\")\n",
    "\n",
    "            ## Collect column and replicate names\n",
    "            merged_colnames = df_merged.columns.tolist()\n",
    "            rep_matches = list(filter(lambda x: re.findall(r\"Rep\\d+\", x), merged_colnames)) ## anonymous function that applies regex to each item in merged_colnames\n",
    "            rep_list = sorted(set(rep_matches)) ## removes duplicate reps and sorts in ascending order\n",
    "\n",
    "            ## Initialize class\n",
    "            filtertsv = FilterTSV()\n",
    "\n",
    "            ## Save merged .tsv (all_sites)\n",
    "            filtertsv.merged_output(df_merged, merged_colnames, rep_list)\n",
    "            df_merged.to_csv(f\"{processed_folder}/{group_name}_all_sites.tsv\", sep = \"\\t\", index = False)\n",
    "\n",
    "            ## Save null .tsv (missing_data)\n",
    "            null_rows = df_merged.isnull().any(axis=1)\n",
    "            df_null = df_merged[null_rows].copy()\n",
    "            df_null.to_csv(f\"{processed_folder}/{group_name}_missing_data.tsv\", sep = \"\\t\", index = False)\n",
    "\n",
    "            ## Save filtered .tsv (filtered)\n",
    "            df_priority = df_merged.dropna()\n",
    "            dfs = filtertsv.priority_output(df_priority, rep_list)\n",
    "            df_filtered = dfs[0]\n",
    "            df_filtered.to_csv(f\"{processed_folder}/{group_name}_filtered.tsv\", sep = \"\\t\", index = False)\n",
    "\n",
    "            ## Save discarded .tsv (non_sites)\n",
    "            df_dropped = dfs[1]\n",
    "            df_dropped.to_csv(f\"{processed_folder}/{group_name}_non_sites.tsv\", sep = \"\\t\", index = False)\n",
    "\n",
    "            # ## Save priority .tsv (priority_filtered)\n",
    "            # \"\"\"\n",
    "            # Drops redundant columns\n",
    "            # \"\"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create merged .tsv file: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7062ddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description = \"Filters .tsv outputs from calculate_dr script.\")\n",
    "    parser.add_argument(\"--folder_name\", help = \"Name of processed_fastqs folder\", required = True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(\"Filtering .tsv files...\")\n",
    "    clean_output(args.folder_name)\n",
    "    print(\"Process finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
