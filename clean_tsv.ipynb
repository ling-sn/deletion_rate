{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "028b3433",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use RNA-STAR conda environment\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.stats import fisher_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4b15cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_regex(folder_name):\n",
    "    \"\"\"\n",
    "    Given input folder names, extract the group name.\n",
    "        EXAMPLE: '7KO-Cyto-BS_processed_fastqs' -> '7KO-Cyto'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        match = re.match(r\"(.+)-(?:BS|NBS)_processed_fastqs\", folder_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to match input folder to group with RegEx: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "    return match.group(1) ## return first capture group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e30fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilterTSV:\n",
    "   def merged_output(df_merged, merged_colnames, rep_list):\n",
    "      \"\"\"\n",
    "      1. Takes all columns from merged df\n",
    "         and organizes them by BS/NBS type \n",
    "      2. Sums up corresponding bases and deletions \n",
    "         and creates 4 new columns per replicate\n",
    "      3. Selects the new columns\n",
    "         * Reshapes each row into 2x2 matrix\n",
    "         * Runs Fisher's Exact Test\n",
    "         * Appends p-value column\n",
    "      \"\"\"\n",
    "      try:\n",
    "         for rep in rep_list: \n",
    "            bs_base_pattern = re.compile(fr\"{rep}_(A|C|G|T)_BS$\")\n",
    "            bs_del_pattern = re.compile(fr\"{rep}_(Deletions)_BS$\")\n",
    "            nbs_base_pattern = re.compile(fr\"{rep}_(A|C|G|T)_NBS$\")\n",
    "            nbs_del_pattern = re.compile(fr\"{rep}_(Deletions)_NBS$\")\n",
    "         \n",
    "            pattern_dict = {f\"{rep}_Bases_BS\": [col for col in merged_colnames if bs_base_pattern.match(col)],\n",
    "                            f\"{rep}_Deletions_BS\": [col for col in merged_colnames if bs_del_pattern.match(col)],\n",
    "                            f\"{rep}_Bases_NBS\": [col for col in merged_colnames if nbs_base_pattern.match(col)],\n",
    "                            f\"{rep}_Deletions_NBS\": [col for col in merged_colnames if nbs_del_pattern.match(col)]}\n",
    "\n",
    "            fisher_cols = [f\"{rep}_TotalBases_BS\", \n",
    "                           f\"{rep}_TotalDeletions_BS\", \n",
    "                           f\"{rep}_TotalBases_NBS\", \n",
    "                           f\"{rep}_TotalDeletions_NBS\"]\n",
    "\n",
    "            for newcol, key in zip(fisher_cols, pattern_dict):\n",
    "               df_merged[newcol] = df_merged[pattern_dict[key]].sum(axis=1) ## new col = sum of list of cols from dictionary\n",
    "\n",
    "            df_merged[f\"{rep}_Pvalue\"] = df_merged[fisher_cols].apply(lambda row: fisher_exact(row.values.reshape(2, 2))[1], axis=1) ## each row is reshaped into 2x2 matrix\n",
    "         \n",
    "         df_merged = df_merged.drop(columns=[\"index\"]) ## after for loop finishes, drop index column\n",
    "      except Exception as e:\n",
    "         print(f\"Failed to calculate p-value for {rep}: {e}\")\n",
    "         traceback.print_exc()\n",
    "         raise\n",
    "    \n",
    "   def priority_output(df_priority, rep_list):\n",
    "      \"\"\"\n",
    "      Adds cutoffs from BID-Pipe protocol:\n",
    "         1. Pvalue for all replicates < 0.0004\n",
    "         2. RealRate for all replicates > 0.3\n",
    "         3. Total sequencing coverage for each BS|NBS replicate > 20\n",
    "         4. Average Deletions across replicate BS > 5\n",
    "         5. Average DeletionRate across replicate BS > 0.02\n",
    "         6. Average DeletionRate across replicate BS is 2x higher than across replicate NBS\n",
    "      \"\"\"\n",
    "      try:\n",
    "         ## Cutoff 1: Pvalue\n",
    "         pval_list = df_priority.columns[df_priority.columns.str.contains(r\"Pvalue$\", regex=True)].tolist()\n",
    "         cutoff1 = df_priority[pval_list].lt(0.0004).all(axis=1)\n",
    "         df_filtered = df_priority[cutoff1]\n",
    "         df_dropped = df_priority[~cutoff1]\n",
    "\n",
    "         ## Cutoff 2: RealRate\n",
    "         realrate_list = df_priority.columns[df_priority.columns.str.contains(r\"RealRate\", regex=True)].tolist()\n",
    "         cutoff2 = df_priority[realrate_list].gt(0.3).all(axis=1)\n",
    "         df_filtered = df_filtered[cutoff2]\n",
    "         df_dropped = pd.concat([df_dropped, df_filtered[~cutoff2]]) ## append dropped rows to existing df\n",
    "\n",
    "         ## Cutoff 3: Total sequencing coverage\n",
    "         for rep in rep_list:\n",
    "            coverage_list = df_priority.columns[df_priority.columns.str.contains(fr\"{rep}_Total.+_BS\", regex=True)].tolist()\n",
    "\n",
    "      except Exception as e:\n",
    "         print(f\"Failed to apply cutoffs from BID-Pipe protocol: {e}\")\n",
    "         traceback.print_exc()\n",
    "         raise\n",
    "\n",
    "   def discarded_output():\n",
    "      \"\"\"\n",
    "      Moves discarded rows from priority_output() into a.tsv file\n",
    "      \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe8a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "## main code\n",
    "def clean_output(folder_name):\n",
    "    \"\"\"\n",
    "    Filters .tsv files in grouped folders\n",
    "    \"\"\"\n",
    "    current_path = Path.cwd()\n",
    "    group_name = match_regex(folder_name)\n",
    "    input_folder = current_path/\"calculations\"/group_name/\"individual_tsv\"\n",
    "    processed_folder = current_path/\"calculations\"/group_name\n",
    "\n",
    "    try: \n",
    "        if input_folder.is_dir():\n",
    "            tsv_list = [*input_folder.glob(\"*.tsv\")] ## collect paths of tsv files and put in a list            \n",
    "            num = [\"df%s\" %s for s in range(1,7)] ## creates a list of strings: df1, df2, ..., df6\n",
    "            listcomp = [pd.read_csv(i, sep = \"\\t\") for i in tsv_list] ## reads in all tsv files as pandas df; access 1st df w/ listcomp[0], etc.\n",
    "            df_dict = dict(zip(num, listcomp))\n",
    "\n",
    "            ## Merge pandas dataframes\n",
    "            colnames = df_dict[\"df1\"].columns.tolist()\n",
    "            selected_colnames = [\"index\"] + colnames[0:17] ## columns that are always the same throughout all dfs\n",
    "\n",
    "            for i in num:\n",
    "                if i=0:\n",
    "                    ## merge df1 + df2\n",
    "                    df_merged = pd.merge(df_dict[i].reset_index(), df_dict[i+1].reset_index(), on = selected_colnames, how = \"outer\")\n",
    "                elif i=5:\n",
    "                    break\n",
    "                else:\n",
    "                    ## for remaining dfs after df1 + df2, overwrite existing var: df_merged = df_merged + df_dict[i+1]\n",
    "                    df_merged = pd.merge(df_merged, df_dict[i+1].reset_index(), on = selected_colnames, how = \"outer\")\n",
    "\n",
    "            ## Collect column and replicate names\n",
    "            merged_colnames = df_merged.columns.tolist()\n",
    "            rep_matches = list(filter(lambda x: re.findall(r\"Rep\\d+\", x), merged_colnames)) ## anonymous function that applies regex to each item in merged_colnames\n",
    "            rep_list = sorted(set(rep_matches)) ## removes duplicate reps and sorts in ascending order\n",
    "\n",
    "            ## Initialize class\n",
    "            filtertsv = FilterTSV()\n",
    "\n",
    "            ## Save merged .tsv (all_sites)\n",
    "            filtertsv.merged_output(df_merged, merged_colnames, rep_list)\n",
    "            df_merged.to_csv(f\"{processed_folder}/{group_name}_all_sites.tsv\", sep = \"\\t\", index = False)\n",
    "\n",
    "            ## Save null .tsv (missing_data)\n",
    "            null_rows = df_merged.isnull().any(axis=1)\n",
    "            df_null = df_merged[null_rows].copy()\n",
    "            df_null.to_csv(f\"{processed_folder}/{group_name}_missing_data.tsv\", sep = \"\\t\", index = False)\n",
    "\n",
    "            ## Save priority .tsv (priority_filtered)\n",
    "            df_priority = df_merged.dropna()\n",
    "            filtertsv.priority_output(df_priority, rep_list)\n",
    "            df_priority.to_csv(f\"{processed_folder}/{group_name}_priority_filtered.tsv\", sep = \"\\t\", index = False)\n",
    "\n",
    "            ## Save discarded .tsv (non_sites)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create merged .tsv file: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7062ddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description = \"Filters .tsv outputs from calculate_dr script.\")\n",
    "    parser.add_argument(\"--folder_name\", help = \"Name of processed_fastqs folder\", required = True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(\"Filtering .tsv files...\")\n",
    "    clean_output(args.folder_name)\n",
    "    print(\"Process finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e44f50d",
   "metadata": {},
   "source": [
    "### Extra code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed9fce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional mean\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "col = \"my_column\"\n",
    "threshold = 10\n",
    "\n",
    "filtered = df[df[col] > threshold]\n",
    "\n",
    "if filtered[col].mean() > threshold:\n",
    "    df = filtered\n",
    "else:\n",
    "    # If you want, keep original df or do something else\n",
    "    pass\n",
    "\n",
    "###################\n",
    "\n",
    "## Fisher's Exact Tests\n",
    "from scipy.stats import fisher_exact\n",
    "\n",
    "# df with columns: 'A', 'B', 'C', 'D'\n",
    "def get_pvalue(row):\n",
    "    table = [[row['A'], row['B']], [row['C'], row['D']]]\n",
    "    _, p = fisher_exact(table)\n",
    "    return p\n",
    "\n",
    "df['p_value'] = df.apply(get_pvalue, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
